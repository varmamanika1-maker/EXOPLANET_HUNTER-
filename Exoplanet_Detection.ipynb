{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OlFQFjSx0iu",
    "outputId": "4deede4f-8ab3-43d3-9be3-b8cc56f9ca57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab. Skipping Google Drive mount.\n"
     ]
    }
   ],
   "source": [
    "# If running in Google Colab, mount Google Drive\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\t# Only import drive if running in Colab\n",
    "\tfrom google.colab import drive  # type: ignore\n",
    "\tdrive.mount('/content/drive')\n",
    "else:\n",
    "\tprint(\"Not running in Google Colab. Skipping Google Drive mount.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6e0yjwsE0ET"
   },
   "source": [
    "Note: Go through the README file to get an idea of the Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLXFUY1JLEVv"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zMtuyegQyJ58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sarthak\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.3.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sarthak\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sarthak\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m \u001b[38;5;66;03m#For converting csv to DataFrame\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#For Plotting Purpose\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msn\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#For Transforming data and evaluation of Models\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "\n",
    "#Tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd #For converting csv to DataFrame\n",
    "\n",
    "#For Plotting Purpose\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "#For Transforming data and evaluation of Models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Hyperparameter tuning of Models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#For calculating time required for Training and Testing of Models\n",
    "import time\n",
    "\n",
    "#Hide Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B98qIaMN8LlE"
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "9PFVzDogy7IR",
    "outputId": "279ced7e-9130-4924-b812-126035341b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LABEL   FLUX.1   FLUX.2   FLUX.3   FLUX.4   FLUX.5   FLUX.6  FLUX.7  \\\n",
      "0      2    93.85    83.81    20.10   -26.98   -39.56  -124.71 -135.18   \n",
      "1      2   -38.88   -33.83   -58.54   -40.09   -79.31   -72.81  -86.55   \n",
      "2      2   532.64   535.92   513.73   496.92   456.45   466.00  464.50   \n",
      "3      2   326.52   347.39   302.35   298.13   317.74   312.70  322.33   \n",
      "4      2 -1107.21 -1112.59 -1118.95 -1095.10 -1057.55 -1034.48 -998.34   \n",
      "\n",
      "    FLUX.8  FLUX.9  ...  FLUX.3188  FLUX.3189  FLUX.3190  FLUX.3191  \\\n",
      "0   -96.27  -79.89  ...     -78.07    -102.15    -102.15      25.13   \n",
      "1   -85.33  -83.97  ...      -3.28     -32.21     -32.21     -24.89   \n",
      "2   486.39  436.56  ...     -71.69      13.31      13.31     -29.89   \n",
      "3   311.31  312.42  ...       5.71      -3.73      -3.73      30.05   \n",
      "4 -1022.71 -989.57  ...    -594.37    -401.66    -401.66    -357.24   \n",
      "\n",
      "   FLUX.3192  FLUX.3193  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197  \n",
      "0      48.57      92.54      39.32      61.42       5.08     -39.54  \n",
      "1      -4.86       0.76     -11.70       6.46      16.00      19.93  \n",
      "2     -20.88       5.06     -11.80     -28.91     -70.02     -96.67  \n",
      "3      20.03     -12.67      -8.77     -17.31     -17.35      13.98  \n",
      "4    -443.76    -438.54    -399.71    -384.65    -411.79    -510.54  \n",
      "\n",
      "[5 rows x 3198 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reading the Training and Testing Data \n",
    "# Note: Update the paths below to the correct location on your system.\n",
    "\n",
    "import os\n",
    "\n",
    "# Update these paths to the correct location of your CSV files\n",
    "train_path = 'C:/Users/sarthak/Downloads/archive/exoTrain.csv'\n",
    "test_path = 'C:/Users/sarthak/Downloads/archive/exoTest.csv'\n",
    "if not os.path.exists(train_path) or not os.path.exists(test_path):\n",
    "    print(\"File(s) not found. Please update the paths above to the correct location on your system.\")\n",
    "else:\n",
    "    exoTrain = pd.read_csv(train_path)\n",
    "    exoTest = pd.read_csv(test_path)\n",
    "    print(exoTrain.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOIUjFE3y9CZ",
    "outputId": "e1e27a5b-3995-48b1-c89c-b9a5ccc7f273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts:\n",
      "LABEL\n",
      "1    5050\n",
      "2      37\n",
      "Name: count, dtype: int64\n",
      "Test label counts:\n",
      "LABEL\n",
      "1    565\n",
      "2      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Printing the count of Labels\n",
    "#1 ---> Not Exoplanet\n",
    "#2 ---> Exoplanet\n",
    "\n",
    "if 'exoTrain' in globals() and 'exoTest' in globals():\n",
    "\tprint(\"Train label counts:\")\n",
    "\tprint(exoTrain['LABEL'].value_counts())\n",
    "\tprint(\"Test label counts:\")\n",
    "\tprint(exoTest['LABEL'].value_counts())\n",
    "else:\n",
    "\tprint(\"exoTrain and/or exoTest are not defined. Please check if the data files exist and are loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgyhf33z8Rb4"
   },
   "source": [
    "# EDA (EXPLORATORY DATA ANALYSIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "B4ao5MfxzLaI"
   },
   "outputs": [],
   "source": [
    "def flux_graph(dataset, row, dataframe, planet):\n",
    "\n",
    "  fig = plt.figure(figsize=(10,5))\n",
    "  ax = fig.add_subplot()\n",
    "  ax.set_title(planet, fontsize=22)\n",
    "  ax.set_xlabel('time', fontsize=17)\n",
    "  ax.set_ylabel('flux_' + str(row), fontsize=17)\n",
    "  ax.grid(False)\n",
    "  if dataframe:\n",
    "    flux_time = list(dataset.columns)\n",
    "    flux_values = dataset[flux_time].iloc[row]\n",
    "  else:\n",
    "    flux_values = dataset[row]\n",
    "\n",
    "  ax.plot(range(1, len(flux_values) + 1),flux_values)\n",
    "  ax.tick_params(colors='black', labelsize=14)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p6T4u2_6zNWF"
   },
   "outputs": [],
   "source": [
    "def display_flux(dataframe, dataset):\n",
    "    with_planet = exoTrain[exoTrain['LABEL'] == 2].head(1).index\n",
    "    wo_planet = exoTrain[exoTrain['LABEL'] == 1].head(1).index\n",
    "\n",
    "    for row in with_planet:\n",
    "        flux_graph(dataset, row, dataframe, planet = 'At least One Exoplanet')\n",
    "    for row in wo_planet:\n",
    "        flux_graph(dataset, row, dataframe, planet = 'No Exoplanet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yRDTghLVzPFQ",
    "outputId": "2ba26f67-7eb9-4255-a9c9-75f6e30972bc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mexoTrain\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m      4\u001b[39m \tdisplay_flux(\u001b[38;5;28;01mTrue\u001b[39;00m, dataset = exoTrain.loc[:, exoTrain.columns != \u001b[33m'\u001b[39m\u001b[33mLABEL\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'exoTrain' in globals():\n",
    "\tdisplay_flux(True, dataset = exoTrain.loc[:, exoTrain.columns != 'LABEL'])\n",
    "else:\n",
    "\tprint(\"exoTrain is not defined. Please ensure the data files exist and are loaded correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Wd9JPRvu8UJb",
    "outputId": "f03d7803-9555-45d6-eaaf-9f95781de639"
   },
   "outputs": [],
   "source": [
    "# PairPlot for first 10 columns\n",
    "if 'exoTrain' in globals():\n",
    "\tsubset_data = exoTrain.iloc[:, :5]\n",
    "\tsn.pairplot(subset_data, hue='LABEL')\n",
    "else:\n",
    "\tprint(\"exoTrain is not defined. Please ensure the data files exist and are loaded correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "g3lIoB9A9tfE",
    "outputId": "feb3f9c2-7307-44d6-dec8-e4621701bf8a"
   },
   "outputs": [],
   "source": [
    "# Define subset_data as in previous EDA cell\n",
    "if 'exoTrain' in globals():\n",
    "\tsubset_data = exoTrain.iloc[:, :5]\n",
    "\tsn.kdeplot(data = subset_data, hue = 'LABEL', x='FLUX.1')\n",
    "else:\n",
    "\tprint(\"exoTrain is not defined. Please ensure the data files exist and are loaded correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRO3a78N-vxj"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfhjmfhYzRBP"
   },
   "outputs": [],
   "source": [
    "def handle_outliers(dataset, num_iterations):\n",
    "    dataset_outlier = dataset\n",
    "\n",
    "    for n in range(num_iterations):\n",
    "        for index, row in dataset_outlier.iterrows():\n",
    "            row_values = row.values\n",
    "            row_max, row_min = row_values.max(), row_values.min()\n",
    "            row_maxidx, row_minidx = row_values.argmax(), row_values.argmin()\n",
    "            row_mean = row_values.mean()\n",
    "\n",
    "            dataset_outlier.iloc[index][row_maxidx] = row_mean\n",
    "\n",
    "            dataset_outlier.iloc[index][row_minidx] = row_mean\n",
    "\n",
    "    return dataset_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hRe2en-zTL3"
   },
   "outputs": [],
   "source": [
    "#Changing the labels from (1 ---> 1) and (2 ---> 0)\n",
    "def change_labels(y_train, y_test):\n",
    "    label_changer = lambda x: 1 if x == 2 else 0\n",
    "    y_train_temp = y_train.apply(label_changer)\n",
    "    y_test_temp = y_test.apply(label_changer)\n",
    "\n",
    "    return y_train_temp, y_test_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35wTyT1X17DG"
   },
   "outputs": [],
   "source": [
    "# Handling the Imbalance of datasets by oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "def RVS(x_train, y_train):\n",
    "    rv = RandomOverSampler(random_state = 42)\n",
    "    x_train_res, y_train_res = rv.fit_resample(x_train, y_train)\n",
    "    return x_train_res, y_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI-lJPmQ0rR5"
   },
   "outputs": [],
   "source": [
    "# Preprocessing training and testing datasets\n",
    "def pre_processing():\n",
    "    x_train, y_train = exoTrain.loc[:, exoTrain.columns != 'LABEL'], exoTrain.loc[:, 'LABEL']\n",
    "    x_test, y_test = exoTest.loc[:, exoTest.columns != 'LABEL'], exoTest.loc[:, 'LABEL']\n",
    "\n",
    "\n",
    "    x_train = handle_outliers(x_train, 5) #Removing the outliers\n",
    "    x_train, y_train = RVS(x_train, y_train) #Upsampling the data using RandomOverSampler\n",
    "    y_train, y_test = change_labels(y_train, y_test) #Changing the labels from (1 --> 1) and (2 --> 0)\n",
    " \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGqedaaM_Tia"
   },
   "outputs": [],
   "source": [
    "# Printing the Confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred):\n",
    "\n",
    "    matrix = confusion_matrix(y_test, y_pred,normalize='true')\n",
    "    df = pd.DataFrame(matrix, columns=[0, 1], index = [0, 1])\n",
    "    df.index.name = 'Real Values'\n",
    "    df.columns.name = 'Predicted Values'\n",
    "    plt.figure(figsize = (10,10)) \n",
    "    sn.heatmap(df, cmap=\"BuGn\", annot=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RByuBOtc_VaB"
   },
   "outputs": [],
   "source": [
    "# Print prediction metrics\n",
    "def display_predictions(y_test, y_pred, y_class_pred, matrix):\n",
    "\n",
    "  #TP ---> TRUE POSITIVE\n",
    "  #TN ---> TRUE NEGATIVE\n",
    "  #FP ---> FALSE POSITIVE\n",
    "  #FN ---> FALSE NEGATIVE\n",
    "  TP = matrix[0][0]\n",
    "  TN = matrix[1][1]\n",
    "  FP = matrix[0][1] \n",
    "  FN = matrix[1][0]\n",
    "\n",
    "\n",
    "\n",
    "  rec = TP/(TP+FN) #Recall\n",
    "  accuracy = (TP+TN)/(TP+FP+TN+FN) #Accuracy\n",
    "  precision = TP/(TP+FP) #Precision\n",
    "  f1 = (2*precision*rec)/(precision+rec) #F1 Score\n",
    "  auc = roc_auc_score(y_test, y_pred) #ROC curve (Area under curve)\n",
    "\n",
    "  print('\\t\\t Prediction Metrics\\n')\n",
    "  print(\"Accuracy:\\t\", \"{:0.4f}\".format(accuracy))\n",
    "  print(\"Precision:\\t\", \"{:0.4f}\".format(precision))\n",
    "  print(\"Recall:\\t\\t\", \"{:0.4f}\".format(rec))\n",
    "  print(\"ROC AUC:\\t\", \"{:0.4f}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUEIJsHOVsRU"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-i-3tmQCjoQ"
   },
   "outputs": [],
   "source": [
    "# Faster Random Forest with RandomizedSearchCV and fewer parameter combinations\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "def find_best_model(model):\n",
    "    # Splitting Dataset\n",
    "    x_train, y_train, x_test, y_test = pre_processing()\n",
    "    x_train, y_train = shuffle(x_train, y_train) # shuffle the data\n",
    "\n",
    "    if(model == 'rf'):\n",
    "        print('Finding the best Random Forest Model (Fast Mode)')\n",
    "        param_dist = { \n",
    "            'n_estimators': [100],\n",
    "            'max_features': ['sqrt'],\n",
    "            'max_depth' : [4],\n",
    "            'criterion' :['gini']\n",
    "        }\n",
    "        md = RandomForestClassifier(random_state=42)\n",
    "\n",
    "        search = RandomizedSearchCV(estimator=md, param_distributions=param_dist, n_iter=1, cv=2, scoring='recall', random_state=42, n_jobs=-1)\n",
    "\n",
    "    elif (model == 'lr'):\n",
    "        param_grid = { \n",
    "            'penalty': ['l2'],\n",
    "            'C':[1],\n",
    "        }\n",
    "        md = LogisticRegression(random_state=42)\n",
    "        search = GridSearchCV(estimator=md, param_grid=param_grid, cv=2, scoring='recall', n_jobs=-1)\n",
    "\n",
    "    elif(model == 'knn'):\n",
    "        param_grid = { \n",
    "            'weights' : ['uniform'],\n",
    "            'n_neighbors':[3],\n",
    "            'p':[2]\n",
    "        }\n",
    "        md = KNeighborsClassifier()\n",
    "        search = GridSearchCV(estimator=md, param_grid=param_grid, cv=2, scoring='recall', n_jobs=-1)\n",
    "\n",
    "    elif(model == 'lightgbm'):\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.1],\n",
    "            'n_estimators': [100],\n",
    "            'max_depth': [6],\n",
    "        }\n",
    "        md = lgb.LGBMClassifier(random_state=42)\n",
    "        search = GridSearchCV(estimator=md, param_grid=param_grid, cv=2, scoring='recall', n_jobs=-1)\n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "    search.fit(x_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    print('Total Time Required : \\n', end-start)\n",
    "\n",
    "    best = search.best_estimator_ #Best Estimator\n",
    "    print('Best Estimator is : \\n')\n",
    "    print(best)\n",
    "    clf = best\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    print('The Confusion Matrix is: \\n')\n",
    "    print(plot_confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print('Classification Report is: \\n')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run a model and get the classifier and predictions for visualization\n",
    "# Modify find_best_model to return clf, x_train, x_test, y_test, y_pred\n",
    "\n",
    "def find_best_model_return(model):\n",
    "    x_train, y_train, x_test, y_test = pre_processing()\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    if model == 'rf':\n",
    "        param_dist = { \n",
    "            'n_estimators': [100],\n",
    "            'max_features': ['sqrt'],\n",
    "            'max_depth' : [4],\n",
    "            'criterion' :['gini']\n",
    "        }\n",
    "        md = RandomForestClassifier(random_state=42)\n",
    "        search = RandomizedSearchCV(estimator=md, param_distributions=param_dist, n_iter=1, cv=2, scoring='recall', random_state=42, n_jobs=-1)\n",
    "    elif model == 'lr':\n",
    "        param_grid = { \n",
    "            'penalty': ['l2'],\n",
    "            'C':[1],\n",
    "        }\n",
    "        md = LogisticRegression(random_state=42)\n",
    "        search = GridSearchCV(estimator=md, param_grid=param_grid, cv=2, scoring='recall', n_jobs=-1)\n",
    "    elif model == 'knn':\n",
    "        param_grid = { \n",
    "            'weights' : ['uniform'],\n",
    "            'n_neighbors':[3],\n",
    "            'p':[2]\n",
    "        }\n",
    "        md = KNeighborsClassifier()\n",
    "        search = GridSearchCV(estimator=md, param_grid=param_grid, cv=2, scoring='recall', n_jobs=-1)\n",
    "    elif model == 'lightgbm':\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.1],\n",
    "            'n_estimators': [100],\n",
    "            'max_depth': [6],\n",
    "        }\n",
    "        md = lgb.LGBMClassifier(random_state=42)\n",
    "        search = GridSearchCV(estimator=md, param_grid=param_grid, cv=2, scoring='recall', n_jobs=-1)\n",
    "    search.fit(x_train, y_train)\n",
    "    best = search.best_estimator_\n",
    "    clf = best\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return clf, x_train, x_test, y_test, y_pred\n",
    "\n",
    "# Example usage:\n",
    "clf, x_train, x_test, y_test, y_pred = find_best_model_return('rf')\n",
    "\n",
    "# --- Visualization and Feature Importance for Tree-Based Models ---\n",
    "importances = None\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    importances = clf.feature_importances_\n",
    "    feature_names = x_train.columns if hasattr(x_train, 'columns') else [f'Feature {i}' for i in range(len(importances))]\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Feature importances are not available for this model.')\n",
    "\n",
    "# --- Visualization: Confusion Matrix Heatmap ---\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization: ROC Curve ---\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "if hasattr(clf, \"predict_proba\"):\n",
    "    y_score = clf.predict_proba(x_test)[:, 1]\n",
    "else:\n",
    "    y_score = clf.decision_function(x_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2Kbzzydr3uO"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-kiLSUnJfO3X",
    "outputId": "2ff403f9-5408-41bb-c401-e3f22ce51a2f"
   },
   "outputs": [],
   "source": [
    "find_best_model('rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5U3QDlSeIOO"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MQbwaltJcjGS",
    "outputId": "b388b6b3-6779-4c6f-e751-14277ad4d354"
   },
   "outputs": [],
   "source": [
    "find_best_model('lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGGSFdlqeOH3"
   },
   "source": [
    "## K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0h_o9zk3cgxD",
    "outputId": "362b04da-a9db-47bc-fff8-d1c7298f94c4"
   },
   "outputs": [],
   "source": [
    "find_best_model('knn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIkhX7sjWatI"
   },
   "source": [
    "## LIGHT GRADIENT BOOSTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "awvJkbnBfMDv",
    "outputId": "734c9149-986d-4457-eab2-df7b5a5abc84"
   },
   "outputs": [],
   "source": [
    "find_best_model('lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1It1oatNWkRy"
   },
   "source": [
    "## Proposed CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBedKkNw_X0p"
   },
   "outputs": [],
   "source": [
    "def cnn_model():\n",
    "\n",
    "    # Data preparation\n",
    "    x_train, y_train, x_test, y_test = pre_processing()\n",
    "    x_train, y_train = shuffle(x_train, y_train) # shuffle the data to avoid stagnant 0.0000e+00 val_accuracy\n",
    "\n",
    "\n",
    "    n_features = x_train.shape[1]\n",
    "\n",
    "    # Architecture\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Reshape((3197, 1), input_shape=(3197,)))\n",
    "    model.add(layers.Normalization())\n",
    "    model.add(layers.Conv1D(filters=11, kernel_size=2, activation='relu', input_shape=(n_features, 1), kernel_regularizer='l2'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv1D(filters=7, kernel_size=2, activation='relu', input_shape=(n_features, 1), kernel_regularizer='l2'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(30, activation=\"relu\"))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(12, activation=\"relu\"))\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Representation of architecture\n",
    "    print(model.summary())\n",
    "\n",
    "    # Compile model\n",
    "    lr_schedule = optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2, decay_steps=10000, decay_rate=0.8)\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=[metrics.Recall()])\n",
    "\n",
    "    # Fit model\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(x_train, y_train, validation_split = 0.2, batch_size=64, callbacks=[early_stop], epochs=30, verbose=2)\n",
    "\n",
    "\n",
    "    #Training Prediction\n",
    "    print('Training Prediction:')\n",
    "    y_class_pred = (model.predict(x_train) > 0.5).astype(\"int32\")\n",
    "    y_pred = model.predict(x_train)\n",
    "\n",
    "    # Calculating the Confustion Matrix\n",
    "    matrix = plot_confusion_matrix(y_train, y_class_pred)\n",
    "\n",
    "    # Displaying the Output Predictions\n",
    "    display_predictions(y_train, y_pred, y_class_pred, matrix)\n",
    "\n",
    "\n",
    "\n",
    "    # Testing Predictions\n",
    "    print('Testing Prediction:')\n",
    "    y_class_pred = (model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Confustion matrix\n",
    "    matrix = plot_confusion_matrix(y_test, y_class_pred)\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    display_predictions(y_test, y_pred, y_class_pred, matrix)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuHAaW0PgaE5"
   },
   "source": [
    "We do not get the best results in the first iteration, but are able to achieve it after multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced Visualization: 3D Scatter Plot of Exoplanet Features ---\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "if 'exoTrain' in globals():\n",
    "    # Select three features for 3D visualization (change as needed)\n",
    "    features_3d = exoTrain.columns[:3] if len(exoTrain.columns) >= 3 else exoTrain.columns\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    xs = exoTrain[features_3d[0]]\n",
    "    ys = exoTrain[features_3d[1]]\n",
    "    zs = exoTrain[features_3d[2]]\n",
    "    labels = exoTrain['LABEL'] if 'LABEL' in exoTrain.columns else np.zeros(len(xs))\n",
    "    scatter = ax.scatter(xs, ys, zs, c=labels, cmap='viridis', s=20)\n",
    "    ax.set_xlabel(features_3d[0])\n",
    "    ax.set_ylabel(features_3d[1])\n",
    "    ax.set_zlabel(features_3d[2])\n",
    "    ax.set_title('3D Scatter Plot of Exoplanet Data')\n",
    "    legend1 = ax.legend(*scatter.legend_elements(), title=\"LABEL\")\n",
    "    ax.add_artist(legend1)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"exoTrain is not defined. Please ensure the data files exist and are loaded correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced Visualization: Interactive 3D Plot with Plotly ---\n",
    "import plotly.express as px\n",
    "if 'exoTrain' in globals():\n",
    "    # Select three features for 3D visualization (change as needed)\n",
    "    features_3d = exoTrain.columns[:3] if len(exoTrain.columns) >= 3 else exoTrain.columns\n",
    "    fig = px.scatter_3d(exoTrain, x=features_3d[0], y=features_3d[1], z=features_3d[2],\n",
    "                        color='LABEL' if 'LABEL' in exoTrain.columns else None,\n",
    "                        title='Interactive 3D Scatter Plot of Exoplanet Data',\n",
    "                        labels={features_3d[0]: features_3d[0], features_3d[1]: features_3d[1], features_3d[2]: features_3d[2]})\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"exoTrain is not defined. Please ensure the data files exist and are loaded correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Advanced Visualization: 3D Surface Plot of Feature Correlations ---\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "if 'exoTrain' in globals():\n",
    "    # Select two features and plot their correlation as a surface\n",
    "    features_2d = exoTrain.columns[:2] if len(exoTrain.columns) >= 2 else exoTrain.columns\n",
    "    z = exoTrain['LABEL'] if 'LABEL' in exoTrain.columns else np.zeros(len(exoTrain))\n",
    "    x = exoTrain[features_2d[0]]\n",
    "    y = exoTrain[features_2d[1]]\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    surf = ax.plot_trisurf(x, y, z, cmap='viridis', edgecolor='none')\n",
    "    ax.set_xlabel(features_2d[0])\n",
    "    ax.set_ylabel(features_2d[1])\n",
    "    ax.set_zlabel('LABEL')\n",
    "    ax.set_title('3D Surface Plot of Feature Correlations')\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"exoTrain is not defined. Please ensure the data files exist and are loaded correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eXWiPuGpba47",
    "outputId": "7beecdf6-d531-462e-f820-5b60e2285759"
   },
   "outputs": [],
   "source": [
    "cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Exoplanet Detection Project\n",
    "\n",
    "This notebook demonstrates the detection of exoplanets using both traditional machine learning models and a Convolutional Neural Network (CNN). The workflow includes:\n",
    "\n",
    "- **Data Loading & Exploration:** Importing and visualizing the dataset, including flux time series for exoplanet and non-exoplanet samples.\n",
    "- **Preprocessing:** Handling outliers, balancing classes with oversampling, and label encoding.\n",
    "- **Model Training:** Training and evaluating Random Forest, Logistic Regression, K-Nearest Neighbors, LightGBM, and a custom CNN model. Hyperparameter tuning is performed for each model.\n",
    "- **Evaluation & Visualization:** Confusion matrices, classification reports, ROC curves, and feature importance plots are provided for model interpretation.\n",
    "- **Key Insights:**\n",
    "  - Tree-based models (Random Forest, LightGBM) provide feature importances for interpretability.\n",
    "  - The CNN model leverages the sequential nature of flux data for improved performance.\n",
    "  - Model performance is evaluated using accuracy, recall, precision, F1-score, and ROC AUC.\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with additional feature engineering or advanced deep learning architectures.\n",
    "- Further optimize hyperparameters for best results.\n",
    "- Explore model ensembling for improved accuracy.\n",
    "\n",
    "_Refer to the README for more details on dataset and methodology._\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
